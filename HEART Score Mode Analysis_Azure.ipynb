{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddafa92f",
   "metadata": {},
   "source": [
    "# HEART Score Subscore-based Mode Analysis\n",
    "\n",
    "Compared to the one-pass HEART score method, in which GPT-3.5-turbo is queried to retrieve all subscores at once and return each subscore and total score, this method aims to evaluate each subscore a multitude of times. GPT will be queried 10 times for each subscore, and the most common output or mode of the subscore will be the subscore used for the final HEART score generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e21603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043c6559",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLD_AZURE_OPENAI_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_ENDPOINT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecile-heart-score-gpt35\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe cow jumped over the \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[43m  \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.9/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.9/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.9/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/openai/lib/python3.9/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again."
     ]
    }
   ],
   "source": [
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "\n",
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "openai.api_key = os.getenv(\"OLD_AZURE_OPENAI_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "\n",
    "response = openai.Completion.create(\n",
    "\n",
    "  engine=\"decile-heart-score-gpt35\",\n",
    "\n",
    "  prompt=\"The cow jumped over the \",\n",
    "\n",
    "  temperature=1,\n",
    "\n",
    "  max_tokens=100,\n",
    "\n",
    "  top_p=0.5,\n",
    "\n",
    "  frequency_penalty=0,\n",
    "\n",
    "  presence_penalty=0,\n",
    "\n",
    "  stop=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeefc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cedab7",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-05-15\" \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = \"decile-heart-score-gpt35-16k\"\n",
    "\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=deployment_name, # The deployment name you chose when you deployed the GPT-35-Turbo or GPT-4 model.\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print(response)\n",
    "\n",
    "# print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49834f75",
   "metadata": {},
   "source": [
    "### Reading in all encounter notes from one pre-compiled txt document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "with open('Test1doc.txt', 'r') as file:\n",
    "    # Read the contents of the file into a variable\n",
    "    file_contents = file.read()\n",
    "\n",
    "# Print the contents of the file\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolateString(ch1,ch2,s):\n",
    "    return s[s.find(ch1)+1:s.find(ch2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860ccd9",
   "metadata": {},
   "source": [
    "### Mode Analysis for History Subscore\n",
    "#### Repeated GPT Analysis of Note #1 for history subscore 10 times. Then, display mode of history subscore to ideally improve accuracy and precision of subscore.\n",
    "\n",
    "Based on all of the patient's notes below, and considering the patient's symptoms and past medical conditions, please choose between the following options to rate their history as: [Slightly suspicious (0)] [Moderately suspicious (1)] [Highly suspicious (2)] [Not enough information to determine History score (9)] Please provide your answer in brackets by choosing between the answers above (e.g. [Slightly suspicious (0)] ). No Prose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78274ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyList = []\n",
    "prompt = \"Based on all of the patient's notes below, and considering the patient's symptoms and past medical conditions, please choose between the following options to rate their history as: [Slightly suspicious (0)] [Moderately suspicious (1)] [Highly suspicious (2)] [Not enough information to determine History score (9)] Please provide your answer in brackets by choosing between the answers above (e.g. [Slightly suspicious (0)] ). No Prose.\"\n",
    "for x in range(0, 9):\n",
    "    messages = [{\"role\": \"user\", \"content\": (prompt + file_contents)}]\n",
    "    response = openai.ChatCompletion.create(engine=deployment_name, messages=messages)\n",
    "    for i, choice in enumerate(response[\"choices\"], start=1):\n",
    "        phrase = choice[\"message\"][\"content\"]\n",
    "        for ch in phrase:\n",
    "            if ch.isnumeric():\n",
    "                historyList.append(ch)\n",
    "        print(choice[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99424c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeHistory = max(set(historyList), key=historyList.count)\n",
    "print(modeHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5324832b",
   "metadata": {},
   "source": [
    "### Mode Analysis for EKG Subscore\n",
    "#### Repeated GPT Analysis of Note #1 for EKG subscore 10 times. Then, display mode of EKG subscore to ideally improve accuracy and precision of subscore.\n",
    "\n",
    "Please consider the patient's EKG findings. Would you categorize it as: [Normal (0)] [Non-specific repolarization disturbance (1)] [Significant ST deviation (2)] [Not enough information to determine EKG score (9)] Additional information: 1 point: No ST deviation but LBBB, LVH, repolarization changes (e.g. digoxin); 2 points: ST deviation not due to LBBB, LVH, or digoxin. Please provide your choice between the answers above in brackets  (e.g. [Normal (0)] ). No Prose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc0425",
   "metadata": {},
   "outputs": [],
   "source": [
    "ekgList = []\n",
    "prompt = \"Please consider the patient's EKG findings. Would you categorize it as: [Normal (0)] [Non-specific repolarization disturbance (1)] [Significant ST deviation (2)] [Not enough information to determine EKG score (9)] Additional information: 1 point: No ST deviation but LBBB, LVH, repolarization changes (e.g. digoxin); 2 points: ST deviation not due to LBBB, LVH, or digoxin. Please provide your choice between the answers above in brackets  (e.g. [Normal (0)] ). No Prose\"\n",
    "for x in range(0, 9):\n",
    "    messages = [{\"role\": \"user\", \"content\": (prompt + file_contents)}]\n",
    "    response = openai.ChatCompletion.create(engine=deployment_name, messages=messages)\n",
    "    for i, choice in enumerate(response[\"choices\"], start=1):\n",
    "        phrase = choice[\"message\"][\"content\"]\n",
    "        for ch in phrase:\n",
    "            if ch.isnumeric():\n",
    "                ekgList.append(ch)\n",
    "        print(choice[\"message\"][\"content\"])\n",
    "modeEKG = max(set(ekgList), key=ekgList.count)\n",
    "print(\"Mode EKG:\", modeEKG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd76982e",
   "metadata": {},
   "source": [
    "### Mode Analysis for Age Subscore\n",
    "#### Repeated GPT Analysis of Note #1 for Age subscore 10 times. Then, display mode of Age subscore to ideally improve accuracy and precision of subscore.\n",
    "\n",
    "Based on the encounter note log below, how would you classify the patient's age: \n",
    "[<45 (0)] [45-64 (1)] [>= 65 (2)]  [Not enough information to determine Age score]\n",
    "\n",
    "Please provide your choice between the answers above in brackets (e.g. [<45 (0)] ). No Prose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c92e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ageList = []\n",
    "def isolateString(ch1,ch2,s):\n",
    "    return s[s.find(ch1)+1:s.find(ch2)]\n",
    "prompt = \"Based on the encounter note log below, how would you classify the patient's age: [<45 (0)] [45-64 (1)] [>= 65 (2)]  [Not enough information to determine Age score] Please provide your choice between the answers above in brackets (e.g. [<45 (0)] ). No Prose.\"\n",
    "for x in range(0, 9):\n",
    "    messages = [{\"role\": \"user\", \"content\": (prompt + file_contents)}]\n",
    "    response = openai.ChatCompletion.create(engine=deployment_name, messages=messages)\n",
    "    for i, choice in enumerate(response[\"choices\"], start=1):\n",
    "        phrase = choice[\"message\"][\"content\"]\n",
    "        ageList.append(isolateString(\"(\", \")\", phrase))\n",
    "        print(choice[\"message\"][\"content\"])\n",
    "modeAge = max(set(ageList), key=ageList.count)\n",
    "print(\"Mode Age:\", modeAge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2aafed",
   "metadata": {},
   "source": [
    "### Mode Analysis for Risk Factors Subscore\n",
    "#### Repeated GPT Analysis of Note #1 for Risk Factors subscore 10 times. Then, display mode of Risk Factors subscore to ideally improve accuracy and precision of subscore.\n",
    "\n",
    "Consider the following risk factors: HTN, hypercholesterolemia, DM, obesity (BMI >30 kg/m²), smoking (current, or smoking cessation ≤3 mo), positive family history (parent or sibling with CVD before age 65); atherosclerotic disease: prior MI, PCI/CABG, CVA/TIA, or peripheral arterial disease. \n",
    "\n",
    "For the patient below, across all the combined encounter notes, how many risk factors are present? \n",
    "[No known risk factors (0)] [1-2 risk factors (1)] [>= 3 risk factors or history of atherosclerotic disease (2)] [Not enough information to determine Risk Factor score (9)]\n",
    "\n",
    "Please provide your final choice, listing one option between the answers above in brackets (e.g. [No known risk factors (0)] ). No Prose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb23be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "riskList = []\n",
    "prompt = \"Consider the following risk factors: HTN, hypercholesterolemia, DM, obesity (BMI >30 kg/m²), smoking (current, or smoking cessation ≤3 mo), positive family history (parent or sibling with CVD before age 65); atherosclerotic disease: prior MI, PCI/CABG, CVA/TIA, or peripheral arterial disease. For the patient below, across all the combined encounter notes, how many risk factors are present? [No known risk factors (0)] [1-2 risk factors (1)] [>= 3 risk factors or history of atherosclerotic disease (2)] [Not enough information to determine Risk Factor score (9)] Please provide your final choice, listing one option between the answers above in brackets (e.g. [No known risk factors (0)] ). No Prose.\"\n",
    "for x in range(0, 9):\n",
    "    messages = [{\"role\": \"user\", \"content\": (prompt + file_contents)}]\n",
    "    response = openai.ChatCompletion.create(engine=deployment_name, messages=messages)\n",
    "    for i, choice in enumerate(response[\"choices\"], start=1):\n",
    "        phrase = choice[\"message\"][\"content\"]\n",
    "        for ch in phrase:\n",
    "            if ch.isnumeric():\n",
    "                riskList.append(ch)\n",
    "        print(choice[\"message\"][\"content\"])\n",
    "modeRisk = max(set(riskList), key=riskList.count)\n",
    "print(\"Mode Risk:\", modeRisk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c96b20",
   "metadata": {},
   "source": [
    "### Mode Analysis for Troponins Subscore\n",
    "#### Repeated GPT Analysis of Note #1 for Troponins subscore 10 times. Then, display mode of Troponins subscore to ideally improve accuracy and precision of subscore.\n",
    "\n",
    "Find the troponin value in based on careful review of all of the encounter note logs provided below. Note that the troponins may be listed without a unit beside it. \n",
    "\n",
    "How would you categorize assessment of the patient's initial troponin measurement:[=< normal limit (0)] [1–3x normal limit (1)] [>3x normal limit (2)] [Not enough information to determine Troponin score (9)] \n",
    "\n",
    "The normal limit for high sensitivity troponin according to Yale New Haven Health is < 12 ng/L. \n",
    "\n",
    "Please provide your choice between the answers above in brackets (e.g. [=< normal limit (0)] ). No Prose.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a32d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tropList = []\n",
    "prompt = \"Find the troponin value based on careful review of all of the encounter note logs provided below. Note that the troponins may be listed without a unit beside it. How would you categorize assessment of the patient's initial troponin measurement:[=< normal limit (0)] [1–3x normal limit (1)] [>3x normal limit (2)] [Not enough information to determine Troponin score (9)] The normal limit for high sensitivity troponin according to Yale New Haven Health is < 12 ng/L. Please provide your choice between the answers above in brackets (e.g. [=< normal limit (0)] ). No Prose. \"\n",
    "for x in range(0, 9):\n",
    "    messages = [{\"role\": \"user\", \"content\": (prompt + file_contents)}]\n",
    "    response = openai.ChatCompletion.create(engine=deployment_name, messages=messages)\n",
    "    for i, choice in enumerate(response[\"choices\"], start=1):\n",
    "        phrase = choice[\"message\"][\"content\"]\n",
    "        tropList.append(isolateString(\"(\", \")\", phrase))\n",
    "        print(choice[\"message\"][\"content\"])\n",
    "modeTrop = max(set(tropList), key=tropList.count)\n",
    "print(\"Mode Troponin:\", modeTrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138d619",
   "metadata": {},
   "source": [
    "### Final HEART Score\n",
    "The final HEART Score after summing each of the individual mode analysis of the subscore prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalHeart = int(modeHistory) + int(modeEKG) + int(modeAge) + int(modeRisk) + int(modeTrop)\n",
    "print(\"Total Heart Score:\", totalHeart)\n",
    "# should be # 2 + 1 + 2 + 2 + 2 = 9?\n",
    "# hmm I got 2 + 2 + 2 + 3 + 2 = 11? \n",
    "# EKG and Risk Factors are different. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
